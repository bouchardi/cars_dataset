{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca79e59a",
   "metadata": {},
   "source": [
    "# Cars Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c65b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch.nn import Identity\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a62d9f",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset should (be downloaded)[https://ai.stanford.edu/~jkrause/cars/car_dataset.html] and extracted into a `data/` folder at the project root.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd87d44",
   "metadata": {},
   "source": [
    "### TASK 1 - Build a function that converts a labelled dataset into labelled and unlabelled subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f47c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labelled_and_unlabelled_indexes(\n",
    "    dataset_labels,\n",
    "    proportion\n",
    "):\n",
    "    if 0 >= proportion >= 1:\n",
    "        raise ValueError(\"`proportion` should be a float between 0 and 1.\")\n",
    "\n",
    "    split_index = int(len(dataset_labels) * (1 - proportion))\n",
    "    unique_classes = np.unique(dataset_labels).tolist()\n",
    "    \n",
    "    if split_index < len(unique_classes):\n",
    "        min_proportion = len(unique_classes) / len(dataset_labels)\n",
    "        raise ValueError(\n",
    "            f\"The proportion should be greater than {min_proportion} to ensure\" \\\n",
    "            \"all unique classes have at least one instance labelled.\"\n",
    "        )\n",
    "        \n",
    "    dataset_indexes = np.arange(0, len(dataset_labels))\n",
    "    \n",
    "    while True:\n",
    "        random.shuffle(dataset_indexes)\n",
    "    \n",
    "        labelled_indexes = dataset_indexes[:split_index]\n",
    "        unlabelled_indexes = dataset_indexes[split_index:]\n",
    "    \n",
    "        # Make sure at least one instance of each class is labelled\n",
    "        if np.unique(np.array(dataset_labels)[labelled_indexes]).tolist() == unique_classes:\n",
    "            break\n",
    "    \n",
    "    return labelled_indexes, unlabelled_indexes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0c644",
   "metadata": {},
   "source": [
    "### TASK 2 - Data cleaning\n",
    "\n",
    "# TODO (PRINT deletion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_RGB_images(image_path):\n",
    "    delete_count = 0\n",
    "    for filename in image_path.iterdir():\n",
    "        image = Image.open(filename)\n",
    "        if image.mode == \"RGB\":\n",
    "            continue\n",
    "            \n",
    "        print(f\"Deleting {filename} (is not an RGB image).\")\n",
    "        filename.unlink()\n",
    "        delete_count += 1\n",
    "\n",
    "    print(f\">>> Deleted {delete_count} files in {image_path}.\")\n",
    "\n",
    "    \n",
    "for image_path in [\n",
    "    DATA_PATH / \"cars_train\", \n",
    "    DATA_PATH / \"cars_test\", \n",
    "]:\n",
    "    delete_non_RGB_images(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e9d05f",
   "metadata": {},
   "source": [
    "### TASK 3 - Dataset representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b63957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_path):\n",
    "    images = {}\n",
    "    for filename in image_path.iterdir():\n",
    "        # Directly load images as tensors\n",
    "        images[filename.name] = read_image(str(filename))\n",
    "    \n",
    "    print(f\"{len(images)} images in {image_path}.\")\n",
    "    \n",
    "    return images    \n",
    "\n",
    "train_images = load_images(DATA_PATH / \"cars_train\")\n",
    "test_images = load_images(DATA_PATH / \"cars_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d183cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotation_files(filename):\n",
    "    data = scipy.io.loadmat(filename)[\"annotations\"][0]\n",
    "    \n",
    "    annotations = {}\n",
    "    for d in data:\n",
    "        image_filename = d[-1][0]\n",
    "        _class = d[-2][0][0]\n",
    "        \n",
    "        annotations[image_filename] = _class\n",
    "    \n",
    "    return annotations\n",
    "        \n",
    "train_annotations = load_annotation_files(DATA_PATH / \"devkit\" / \"cars_train_annos.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    # Replace last layer\n",
    "    model.fc = Identity()\n",
    "    \n",
    "    return model.eval()\n",
    "\n",
    "model = prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(model, transform, images, annotations):\n",
    "    dataset = {}\n",
    "    for i, (image_filename, image) in enumerate(tqdm(images.items())):\n",
    "        if image_filename not in annotations:\n",
    "            print(image_filename)\n",
    "            continue\n",
    "        \n",
    "        x = image / 256\n",
    "        x = transform(x)\n",
    "        x = x.float()\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        embedding = model(x).detach()\n",
    "        embedding = embedding[0].numpy()\n",
    "        \n",
    "        dataset[i] = {\n",
    "            \"embedding\": embedding, \n",
    "            \"class_idx\": annotations[image_filename], \n",
    "            \"labelled\": True\n",
    "        }\n",
    "        \n",
    "    return dataset\n",
    "    \n",
    "dataset = prepare_dataset(\n",
    "    model=model, \n",
    "    transform=ResNet18_Weights.DEFAULT.transforms(), \n",
    "    images=train_images, \n",
    "    annotations=train_annotations\n",
    ")\n",
    "torch.save(dataset, DATA_PATH / \"dataset.pt\")\n",
    "\n",
    "# dataset = torch.load(DATA_PATH / \"dataset.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73f022",
   "metadata": {},
   "source": [
    "### TASK 4 - Build a partially labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inputs, dataset_labels = zip(*[\n",
    "    (d[\"embedding\"], d[\"class_idx\"]) for d in dataset.values()\n",
    "])\n",
    "    \n",
    "labelled_indexes, unlabelled_indexes = get_labelled_and_unlabelled_indexes(\n",
    "    dataset_labels,\n",
    "    0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b3b8d",
   "metadata": {},
   "source": [
    "### TASK 5 - Create train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(\n",
    "    dataset_inputs,\n",
    "    dataset_labels,\n",
    "    training_proportion\n",
    "):\n",
    "    if 0 >= training_proportion >= 1:\n",
    "        raise ValueError(\"`training_proportion` should be a float between 0 and 1.\")\n",
    "    \n",
    "    index_split = int(len(dataset_inputs) * training_proportion)\n",
    "    \n",
    "    training_inputs = dataset_inputs[:index_split]\n",
    "    training_labels = dataset_labels[:index_split]\n",
    "    print(f\"Train: {len(training_inputs)} samples\")\n",
    "    \n",
    "    valid_inputs = dataset_inputs[index_split:]\n",
    "    valid_labels = dataset_labels[index_split:]\n",
    "    print(f\"Valid: {len(valid_inputs)} samples\")\n",
    "    \n",
    "    return training_inputs, training_labels, valid_inputs, valid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b307b",
   "metadata": {},
   "source": [
    "### TASK 6 - Create experiment(s) to convince clients that more labelled data will improve model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def select_items(my_list, selected_indexes):\n",
    "    selected_items = [\n",
    "        item for i, item in enumerate(my_list) if i in selected_indexes\n",
    "    ]\n",
    "    \n",
    "    return selected_items\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, max_iteration=1000, verbose=0):\n",
    "    classifier = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SGDClassifier(loss=\"perceptron\", max_iter=max_iteration,  tol=1e-3, penalty=\"l2\", verbose=verbose)\n",
    "    )\n",
    "    \n",
    "    t0 = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    print(f\"Classifier trained in {round(t1 - t0, 2)} seconds.\")\n",
    "    \n",
    "    score = classifier.score(X_test, y_test)\n",
    "    \n",
    "    return classifier, score\n",
    "\n",
    "\n",
    "dataset_inputs_labelled = select_items(dataset_inputs, labelled_indexes)\n",
    "dataset_labels_labelled = select_items(dataset_labels, labelled_indexes)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = split_dataset(\n",
    "    dataset_inputs_labelled,\n",
    "    dataset_labels_labelled,\n",
    "    training_proportion=0.8\n",
    ")\n",
    "\n",
    "# ratios = np.arange(0.1, 1, 0.1)\n",
    "ratios = [1]\n",
    "scores = []\n",
    "\n",
    "for ratio in ratios:\n",
    "    split_index = int(len(X_train) * ratio)\n",
    "    X_subset_train = X_train[:split_index]\n",
    "    y_subset_train = y_train[:split_index]\n",
    "    \n",
    "    classifier, score = train_model(\n",
    "        X_subset_train, \n",
    "        y_subset_train, \n",
    "        X_valid, \n",
    "        y_valid, \n",
    "        max_iteration=1000, \n",
    "        verbose=0\n",
    "    )\n",
    "    scores.append(score)\n",
    "\n",
    "print(score)\n",
    "# plt.plot(ratios, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd0e76e",
   "metadata": {},
   "source": [
    "### TASK 7 - Active learning to select new instances to be labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dceb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_candidates = select_items(dataset_inputs, unlabelled_indexes)\n",
    "y_candidates = select_items(dataset_labels, unlabelled_indexes)\n",
    "\n",
    "K_candidates = int(len(dataset_inputs) * 0.25)\n",
    "\n",
    "probabilities = classifier.predict_proba(X_candidates)\n",
    "entropies = scipy.stats.entropy(probabilities)\n",
    "\n",
    "selected_candidate_indexes = np.argsort(entropies).tolist()[-K_candidates:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282aec2",
   "metadata": {},
   "source": [
    "### TASK 8 - Final model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c376f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_extra = select_items(dataset_inputs, selected_candidate_indexes)\n",
    "y_extra = select_items(dataset_labels, selected_candidate_indexes)\n",
    "\n",
    "X_final = X_train + X_extra\n",
    "y_final = y_train + y_extra\n",
    "\n",
    "classifier.fit(X_final, y_final)\n",
    "\n",
    "# We want our validation set to remain the same\n",
    "score = classifier.score(X_valid, y_valid)\n",
    "\n",
    "print(f\"FINAL SCORE {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
